{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svkBnr6cG2fv"
      },
      "outputs": [],
      "source": [
        "def having_ip_address(url):\n",
        "  import re\n",
        "  x = re.search('^(http|https)://\\d+\\.\\d+\\.\\d+\\.\\d+\\.*', url)\n",
        "  if x:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "having_ip_address('https://128.36.54.192/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "metadata": {
        "id": "td13py-FG7y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def URL_Length(url):\n",
        "  if len(url) < 54:\n",
        "    return -1\n",
        "  elif len(url) >= 54 and len(url) <=75:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "lZfLjPlPG70H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL_Length('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "metadata": {
        "id": "YHpBj306G72U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Module 3"
      ],
      "metadata": {
        "id": "81kF19fgG73Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def haveAtSign(url):\n",
        "  if \"@\" in url:\n",
        "    at = 1    \n",
        "  else:\n",
        "    at = -1    \n",
        "  return at"
      ],
      "metadata": {
        "id": "gUDurmljG75_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "haveAtSign('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "metadata": {
        "id": "dvRZhlj6G766"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prefix_Suffix"
      ],
      "metadata": {
        "id": "moYHQAYnG7-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prefixSuffix(url):\n",
        "    from urllib.parse import urlparse\n",
        "    if '-' in urlparse(url).netloc:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "QzFJ82PlG7_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefixSuffix('https://dfhdfhdfgs.tokyo/ja-jp/account/login')"
      ],
      "metadata": {
        "id": "VEELA6erG8CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Having Sub Domain"
      ],
      "metadata": {
        "id": "51lshesiG8DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_domain_count(url):\n",
        "  !pip install tld\n",
        "  from urllib.parse import urlparse\n",
        "  from tld import get_tld\n",
        "  domain = urlparse(url).netloc\n",
        "  domain = domain.split('.')\n",
        "  top_domain = get_tld(url)\n",
        "  top_domain = top_domain.split('.')\n",
        "  sub_domain = set(domain) - set(top_domain)\n",
        "  count = len(sub_domain)\n",
        "  if(count == 1):\n",
        "    return -1\n",
        "  if(count == 2):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1 "
      ],
      "metadata": {
        "id": "LWxcCmIjG8F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_domain_count('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "metadata": {
        "id": "hvhErdmxG8G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###SSL final state"
      ],
      "metadata": {
        "id": "abyg8HXoG8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sslVerify(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from urllib.request import Request, urlopen, ssl, socket\n",
        "  import json\n",
        "  from datetime import datetime\n",
        "  split_url = urlsplit(url)\n",
        "  #some site without http/https in the path\n",
        "  port = '443'\n",
        "\n",
        "  hostname = split_url.netloc\n",
        "  context = ssl.create_default_context()\n",
        "  try:\n",
        "    with socket.create_connection((hostname, port)) as sock:\n",
        "      with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
        "          data = json.dumps(ssock.getpeercert())\n",
        "          res = json.loads(data)\n",
        "    notBefore = datetime.strptime(res[\"notBefore\"],'%b  %d %H:%M:%S %Y %Z').date()\n",
        "    notAfter = datetime.strptime(res[\"notAfter\"],'%b  %d %H:%M:%S %Y %Z').date()\n",
        "    # print(notBefore, notAfter)\n",
        "    if(ssl.SSLCertVerificationError(ssock) == True):\n",
        "      return 1\n",
        "    elif (notAfter.year-notBefore.year)+(notAfter.month-notBefore.month)*0.1 >= 1:\n",
        "      return -1\n",
        "    else:\n",
        "      return 0\n",
        "  except:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "Aa1x2WRxG8K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sslVerify(\"http://postdebanks.com/DIE/POST/diepost/\")"
      ],
      "metadata": {
        "id": "PjTXNXTXG8NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#port"
      ],
      "metadata": {
        "id": "wCKzI4AbG8OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def port(domain):\n",
        "  import requests\n",
        "  import json\n",
        "  response = requests.get(\"https://api.viewdns.info/portscan/?host=\"+domain+\"&apikey=1bf03196763a201bcc66a59bf88ed8ddf7a9432f&output=json\")\n",
        "  myjson = response.json()\n",
        "  # print(myjson)\n",
        "  pref_stat = {21:'closed', 22:'closed', 23:'closed', 80:'open', 443:'open', 445 : 'closed', 1433:'closed', 1521:'closed', 3306 : 'closed', 3389:'closed' }\n",
        "  flag = -1\n",
        "  for i in range(len(myjson['response']['port'])):\n",
        "    if int(myjson['response']['port'][i]['number']) in pref_stat:\n",
        "      if(myjson['response']['port'][0]['status'] != pref_stat[int(myjson['response']['port'][0]['number'])]):\n",
        "        flag = 1\n",
        "  return flag"
      ],
      "metadata": {
        "id": "ycbAcwrJG8RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# port('postdebanks.com')"
      ],
      "metadata": {
        "id": "QiPOk09PG8SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Request_URL"
      ],
      "metadata": {
        "id": "zxBLj1ptG8VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def request_url(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        print('Yes')\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  else:\n",
        "        print('No state')\n",
        "  \n",
        "  Media = {'internals':[], 'externals':[], 'null':[]}\n",
        "\n",
        "  def external_media(Media):\n",
        "    total = len(Media['internals']) + len(Media['externals'])\n",
        "    externals = len(Media['externals'])\n",
        "    try:\n",
        "        percentile = externals / float(total) * 100\n",
        "    except:\n",
        "        return 0\n",
        "    return percentile\n",
        "  \n",
        "  def findMedia(Media,domain, hostname ):\n",
        "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
        "    for img in soup.find_all('img', src=True):\n",
        "          dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
        "          if hostname in img['src'] or domain in img['src'] or len(dots) == 1 or not img['src'].startswith('http'):\n",
        "              if not img['src'].startswith('http'):\n",
        "                  if not img['src'].startswith('/'):\n",
        "                      Media['internals'].append(hostname+'/'+img['src']) \n",
        "                  elif img['src'] in Null_format:\n",
        "                      Media['null'].append(img['src'])  \n",
        "                  else:\n",
        "                      Media['internals'].append(hostname+img['src'])   \n",
        "          else:\n",
        "              Media['externals'].append(img['src'])\n",
        "    for audio in soup.find_all('audio', src=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
        "      if hostname in audio['src'] or domain in audio['src'] or len(dots) == 1 or not audio['src'].startswith('http'):\n",
        "          if not audio['src'].startswith('http'):\n",
        "              if not audio['src'].startswith('/'):\n",
        "                  Media['internals'].append(hostname+'/'+audio['src']) \n",
        "              elif audio['src'] in Null_format:\n",
        "                  Media['null'].append(audio['src'])  \n",
        "              else:\n",
        "                  Media['internals'].append(hostname+audio['src'])   \n",
        "      else:\n",
        "          Media['externals'].append(audio['src'])\n",
        "          \n",
        "    for embed in soup.find_all('embed', src=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', embed['src'])]\n",
        "      if hostname in embed['src'] or domain in embed['src'] or len(dots) == 1 or not embed['src'].startswith('http'):\n",
        "          if not embed['src'].startswith('http'):\n",
        "              if not embed['src'].startswith('/'):\n",
        "                  Media['internals'].append(hostname+'/'+embed['src']) \n",
        "              elif embed['src'] in Null_format:\n",
        "                  Media['null'].append(embed['src'])  \n",
        "              else:\n",
        "                  Media['internals'].append(hostname+embed['src'])   \n",
        "      else:\n",
        "          Media['externals'].append(embed['src'])\n",
        "        \n",
        "    for i_frame in soup.find_all('iframe', src=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', i_frame['src'])]\n",
        "      if hostname in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1 or not i_frame['src'].startswith('http'):\n",
        "          if not i_frame['src'].startswith('http'):\n",
        "              if not i_frame['src'].startswith('/'):\n",
        "                  Media['internals'].append(hostname+'/'+i_frame['src']) \n",
        "              elif i_frame['src'] in Null_format:\n",
        "                  Media['null'].append(i_frame['src'])  \n",
        "              else:\n",
        "                  Media['internals'].append(hostname+i_frame['src'])   \n",
        "      else: \n",
        "          Media['externals'].append(i_frame['src'])\n",
        "  \n",
        "  findMedia(Media, domain, hostname)\n",
        "  if external_media(Media) < 22:\n",
        "    return -1\n",
        "  elif external_media(Media) >= 22 and external_media(Media) < 61:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "wprbElrDG8WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# request_url('http://www.budgetbots.com/server.php/Server%20update/index.php?email=USER@DOMAIN.com')"
      ],
      "metadata": {
        "id": "PR1hSIbUG8Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Safe anchors"
      ],
      "metadata": {
        "id": "hwPmMCm9G8ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def url_anchor(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "    \n",
        "  Anchor = {'safe':[], 'unsafe':[], 'null':[]}\n",
        "\n",
        "  def anchor(Anchor, domain, hostname):\n",
        "      soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
        "      for href in soup.find_all('a', href=True):\n",
        "        dots = [x.start(0) for x in re.finditer('\\.', href['href'])]\n",
        "        if hostname in href['href'] or domain in href['href'] or len(dots) == 1 or not href['href'].startswith('http'):\n",
        "            if \"#\" in href['href'] or \"javascript\" in href['href'].lower() or \"mailto\" in href['href'].lower():\n",
        "                 Anchor['unsafe'].append(href['href'])\n",
        "        else:\n",
        "            Anchor['safe'].append(href['href'])\n",
        "\n",
        "  anchor(Anchor, domain, url)\n",
        "\n",
        "  def safe_anchor(Anchor):\n",
        "      total = len(Anchor['safe']) +  len(Anchor['unsafe'])\n",
        "      unsafe = len(Anchor['unsafe'])\n",
        "      try:\n",
        "        percentile = unsafe / float(total) * 100\n",
        "      except:\n",
        "        return 0\n",
        "      return percentile\n",
        "  #print(safe_anchor(Anchor))   \n",
        "  if safe_anchor(Anchor) < 31:\n",
        "       return -1\n",
        "  elif safe_anchor(Anchor) >= 31 and safe_anchor(Anchor) <= 67:\n",
        "       return 0\n",
        "  else:\n",
        "       return 1"
      ],
      "metadata": {
        "id": "4loY4UCgG8eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_anchor('https://www.xiaoji.com/')"
      ],
      "metadata": {
        "id": "UE3D8J3gG8e7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}