{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL2dk2hdLPUa"
      },
      "outputs": [],
      "source": [
        "#Module 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def links_tag(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  Link = {'internals':[], 'externals':[], 'null':[]}\n",
        "\n",
        "  def find_links(Link, domain, hostname):\n",
        "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
        "    for link in soup.findAll('link', href=True):\n",
        "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
        "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
        "            if not link['href'].startswith('http'):\n",
        "                if not link['href'].startswith('/'):\n",
        "                    Link['internals'].append(hostname+'/'+link['href']) \n",
        "                elif link['href'] in Null_format:\n",
        "                    Link['null'].append(link['href'])  \n",
        "                else:\n",
        "                    Link['internals'].append(hostname+link['href'])   \n",
        "        else:\n",
        "            Link['externals'].append(link['href'])\n",
        "\n",
        "    for script in soup.find_all('script', src=True):\n",
        "        dots = [x.start(0) for x in re.finditer('\\.', script['src'])]\n",
        "        if hostname in script['src'] or domain in script['src'] or len(dots) == 1 or not script['src'].startswith('http'):\n",
        "            if not script['src'].startswith('http'):\n",
        "                if not script['src'].startswith('/'):\n",
        "                    Link['internals'].append(hostname+'/'+script['src']) \n",
        "                elif script['src'] in Null_format:\n",
        "                    Link['null'].append(script['src'])  \n",
        "                else:\n",
        "                    Link['internals'].append(hostname+script['src'])   \n",
        "        else:\n",
        "            Link['externals'].append(link['href'])\n",
        "  \n",
        "  def links_in_tags(Link):\n",
        "    total = len(Link['internals']) +  len(Link['externals'])\n",
        "    internals = len(Link['internals'])\n",
        "    try:\n",
        "        percentile = internals / float(total) * 100\n",
        "    except:\n",
        "        return 0\n",
        "    return percentile\n",
        "  #print(links_in_tags(Link))\n",
        "  if links_in_tags(Link) < 17:\n",
        "    return -1\n",
        "  elif links_in_tags(Link) >= 17 and links_in_tags(Link) <= 81:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "7nvMVY3OLQO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links_tag('https://www.xiaoji.com/')"
      ],
      "metadata": {
        "id": "317HXLB0LQQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###SFH"
      ],
      "metadata": {
        "id": "JZgHDaL5LQSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sfh(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  Form = {'internals':[], 'externals':[], 'null':[]}\n",
        "\n",
        "  def findForm(Form, domain, hostname):\n",
        "    for form in soup.findAll('form', action=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
        "      if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
        "          if not form['action'].startswith('http'):\n",
        "              if not form['action'].startswith('/'):\n",
        "                  Form['internals'].append(hostname+'/'+form['action']) \n",
        "              elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
        "                  Form['null'].append(form['action'])  \n",
        "              else:\n",
        "                  Form['internals'].append(hostname+form['action'])   \n",
        "      else:\n",
        "          Form['externals'].append(form['action'])\n",
        "  \n",
        "  def sf(hostname, Form):\n",
        "    if len(Form['null'])==0:\n",
        "        return 1\n",
        "    elif len(Form['null'])>0:\n",
        "        return 0\n",
        "    else:\n",
        "      return -1\n",
        "  \n",
        "  return(sf(hostname, Form))"
      ],
      "metadata": {
        "id": "xtUSXDj2LQTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sfh('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "metadata": {
        "id": "I1vUlJ1fLQWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submitting to emial"
      ],
      "metadata": {
        "id": "hfVuxE9mLQW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_email(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  Form = {'internals':[], 'externals':[], 'null':[]}\n",
        "  def findForm(Form, domain, hostname):\n",
        "    for form in soup.findAll('form', action=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
        "      if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
        "          if not form['action'].startswith('http'):\n",
        "              if not form['action'].startswith('/'):\n",
        "                  Form['internals'].append(hostname+'/'+form['action']) \n",
        "              elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
        "                  Form['null'].append(form['action'])  \n",
        "              else:\n",
        "                  Form['internals'].append(hostname+form['action'])   \n",
        "      else:\n",
        "          Form['externals'].append(form['action'])\n",
        "  \n",
        "  def submitting_to_email(Form):\n",
        "    # print(Form)\n",
        "    for form in (Form['internals'] + Form['externals']):\n",
        "        #print('inside for')\n",
        "        if \"mailto:\" in form or \"mail()\" in form:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    return 1\n",
        "  \n",
        "  return(submitting_to_email(Form))"
      ],
      "metadata": {
        "id": "8kVSAwFaLQaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('ans is ',  sub_email('https://www.xiaoji.com/'))"
      ],
      "metadata": {
        "id": "8iXUsIu7LQbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#On Mouse-over"
      ],
      "metadata": {
        "id": "NO076ShTLQeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mouse_over(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  def onmouseover(content):\n",
        "    if 'onmouseover=\"window.status=' in str(content).lower().replace(\" \",\"\"):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  return(onmouseover(content.decode('latin-1')))"
      ],
      "metadata": {
        "id": "BZJPggvSLQfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mouse_over('https://www.xiaoji.com/')"
      ],
      "metadata": {
        "id": "HwAESdBDLQiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Right Click"
      ],
      "metadata": {
        "id": "ryBbDjKeLQjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def right_click(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  def right_clic(content):\n",
        "    if re.findall(r\"event.button ?== ?2\", content.decode('latin-1')):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  \n",
        "  return right_clic(content)"
      ],
      "metadata": {
        "id": "xgTKD9dYLQmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "right_click('http://walletconnectbits.com/')"
      ],
      "metadata": {
        "id": "gvHr2idwLQnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Domain Age"
      ],
      "metadata": {
        "id": "0X8Tl5WDLQqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def domain_age(domain):\n",
        "    import json\n",
        "    import requests\n",
        "    url = domain.split(\"//\")[-1].split(\"/\")[0].split('?')[0]\n",
        "    show = \"https://input.payapi.io/v1/api/fraud/domain/age/\" + url\n",
        "    r = requests.get(show)\n",
        "\n",
        "    if r.status_code == 200:\n",
        "        data = r.text\n",
        "        jsonToPython = json.loads(data)\n",
        "        result = jsonToPython['result']\n",
        "        if result/30 >= 6:\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "    else:       \n",
        "        return -1"
      ],
      "metadata": {
        "id": "j4G4yJWgLQrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domain_age('http://walletconnectbits.com/')"
      ],
      "metadata": {
        "id": "UZruD53OLQwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DNS Record"
      ],
      "metadata": {
        "id": "W5V2tUQmLQxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dns_record(domain):\n",
        "    !pip install dnspython\n",
        "    import dns.resolver\n",
        "    try:\n",
        "        nameservers = dns.resolver.resolve(domain,'NS')\n",
        "        if len(nameservers) > 0:\n",
        "            #print('len is', nameservers)\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "rf5d-JNWLQ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dns_record('http://postdebanks.com/DIE/POST/diepost/')"
      ],
      "metadata": {
        "id": "3u9Nl2UiLQ1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Website Traffic"
      ],
      "metadata": {
        "id": "Boe5Yry-LQ4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_traffic(url):\n",
        "  from urllib.parse import quote\n",
        "  from urllib.request import urlopen\n",
        "  from bs4 import BeautifulSoup\n",
        "  try:\n",
        "    #Filling the whitespaces in the URL if any\n",
        "    url = quote(url)\n",
        "    rank = BeautifulSoup(urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\n",
        "        \"REACH\")['RANK']\n",
        "    rank = int(rank)\n",
        "  except TypeError:\n",
        "        return 1\n",
        "  if rank <= 100000:\n",
        "    return -1\n",
        "  elif rank > 100000:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "nlxpFhPeLQ5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#web_traffic('http://postdebanks.com/DIE/POST/diepost/')"
      ],
      "metadata": {
        "id": "WvRfUGzuLQ9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Page Rank"
      ],
      "metadata": {
        "id": "BUbsshn2LQ99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def page_rank(domain):\n",
        "    import requests\n",
        "    key = '8o0gg0804g4k0gwkk4oocws04oc0sg88gg844o4k'\n",
        "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + str(domain)\n",
        "    try:\n",
        "        request = requests.get(url, headers={'API-OPR': key})\n",
        "        result = request.json()\n",
        "        result = result['response'][0]['page_rank_integer']\n",
        "        if result / 10 < 0.2 :\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    except:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "z-iy1uh-LRBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#page_rank('http://postdebanks.com/DIE/POST/diepost/')"
      ],
      "metadata": {
        "id": "RrHbe0uYLRB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Google Index"
      ],
      "metadata": {
        "id": "NwK0hUF7LRFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def google_index(url):\n",
        "    from urllib.parse import urlencode\n",
        "    from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "    #time.sleep(.6)\n",
        "    user_agent =  'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
        "    headers = {'User-Agent' : user_agent}\n",
        "    query = {'q': 'site:' + url}\n",
        "    google = \"https://www.google.com/search?\" + urlencode(query)\n",
        "    data = requests.get(google, headers=headers)\n",
        "    data.encoding = 'ISO-8859-1'\n",
        "    soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
        "    try:\n",
        "      if 'Our systems have detected unusual traffic from your computer network.' in str(soup):\n",
        "        return -1\n",
        "      check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"a\")\n",
        "      #print(soup.prettify())\n",
        "      if check and check['href']:\n",
        "        return -1\n",
        "      else:\n",
        "        return 1\n",
        "        \n",
        "    except AttributeError:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "ZDYOtlIuLRGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#google_index('http://walletconnectbits.com/')"
      ],
      "metadata": {
        "id": "7a4y5Lg3LRJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Links pointing to page (No of Hyperlinks)"
      ],
      "metadata": {
        "id": "h5-iKSO9LRKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def urlsCount(url):\n",
        "  from bs4 import BeautifulSoup\n",
        "  from collections import Counter\n",
        "  import requests\n",
        "  soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
        "  foundUrls = Counter([link[\"href\"] for link in soup.find_all(\"a\", href=lambda href: href and not href.startswith(\"#\"))])\n",
        "  count = len(foundUrls)\n",
        "  if(count == 0):\n",
        "    return 1\n",
        "  elif(count > 0 and count <= 2):\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ],
      "metadata": {
        "id": "IjMLNOozLRNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#urlsCount('https://www.hokabutypolska.pl/')"
      ],
      "metadata": {
        "id": "Kvck9jPvLROs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#statistical report"
      ],
      "metadata": {
        "id": "2eX1dLG3LRRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def statistical_report(url, domain):\n",
        "    import re\n",
        "    import socket\n",
        "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
        "    try:\n",
        "        ip_address=socket.gethostbyname(domain)\n",
        "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
        "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
        "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
        "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
        "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
        "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
        "        if url_match or ip_match:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    except:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "_Sm6TbEJLRS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistical_report('http://postdebanks.com/DIE/POST/diepost/', 'postdebanks.com')"
      ],
      "metadata": {
        "id": "46KxOs_kLRVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input Features"
      ],
      "metadata": {
        "id": "ymyQEmw0LRWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getInput(url):\n",
        "  from urllib.parse import urlparse\n",
        "  domain = urlparse(url).netloc\n",
        "  input = []\n",
        "  print(domain)\n",
        "  input.append(having_ip_address(url))\n",
        "  input.append(URL_Length(url))\n",
        "  input.append(haveAtSign(url))\n",
        "  input.append(prefixSuffix(url))\n",
        "  input.append(sub_domain_count(url))\n",
        "  input.append(sslVerify(url))\n",
        "  input.append(port(domain))\n",
        "  input.append(request_url(url))\n",
        "  input.append(url_anchor(url))\n",
        "  input.append(links_tag(url))\n",
        "  input.append(sfh(url))\n",
        "  input.append(sub_email(url))\n",
        "  input.append(mouse_over(url))\n",
        "  input.append(right_click(url))\n",
        "  input.append(domain_age(url))\n",
        "  input.append(dns_record(url))\n",
        "  input.append(web_traffic(url))\n",
        "  input.append(page_rank(domain))\n",
        "  input.append(google_index(url))\n",
        "  input.append(urlsCount(url))\n",
        "  input.append(statistical_report(url, domain))\n",
        "  return (input)"
      ],
      "metadata": {
        "id": "u1nIuqtQLRZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = getInput('https://post-u8719-sufficient-159.sites.qsandbox.com/swisse/Home/')\n",
        "input"
      ],
      "metadata": {
        "id": "9YBVQVPfLRaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[input]"
      ],
      "metadata": {
        "id": "5EhcC1HPNm0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = '/content/finalized_model.sav'\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "8QdMYIY4Nm1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "5oGYWOQ0Nm4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = loaded_model.predict([input])\n",
        "if result == -1:\n",
        "  print(\"A legitimate website\")\n",
        "else:\n",
        "  print(\"A Phishing website!!\")"
      ],
      "metadata": {
        "id": "SXds8fN1Nm5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}